# Azure OpenAI instance connection configuration
# Externally defined in environment variables

# Set the model temperature for deterministic responses
quarkus.langchain4j.azure-openai.chat-model.temperature=0.1
# An alternative (or a complement?) to temperature: 0.1 means only top 10% probable tokens are considered
quarkus.langchain4j.azure-openai.chat-model.top-p=0.1
%dev.quarkus.langchain4j.azure-openai.chat-model.log-requests=true
%dev.quarkus.langchain4j.azure-openai.chat-model.log-responses=false
%test.quarkus.langchain4j.azure-openai.chat-model.log-responses=true

# quarkus.micrometer.export.json.enabled=true
# quarkus.micrometer.export.json.path=metrics/json
# quarkus.micrometer.export.prometheus.path=metrics/prometheus

# quarkus.langchain4j.chat-model.provider=azure-openai
# Check consistency with @RegisterAiService fault tolerance configuration
quarkus.langchain4j.azure-openai.max-retries=3
quarkus.langchain4j.azure-openai.timeout=2m

# Directory to store RAG documents
app.docs-for-rag.dir=docs-for-rag

# Required for EmbeddingModel injection in DocInjestor and DocRetriever
# quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.AllMiniLmL6V2EmbeddingModel
quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel

# Disable console input in dev mode to avoid blocking the app
quarkus.console.disable-input=true

quarkus.log.category."io.jefrajames".level=DEBUG